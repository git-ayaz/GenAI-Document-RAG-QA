# ğŸ“„ Multi-Document RAG Chatbot

A **Retrieval-Augmented Generation (RAG) chatbot** that allows users to upload **multiple PDF documents** and ask **context-aware questions** with **chat memory**.  
The system uses a **FastAPI backend**, **Streamlit frontend**, **ChromaDB** for vector storage, and **Groq LLMs** for fast inference.

---

## ğŸš€ Features

- ğŸ“‚ Upload and index **multiple PDF documents**
- ğŸ” Ask questions **specific to a selected document**
- ğŸ§  **Chat memory** for multi-turn conversations
- âš¡ Fast inference using **Groq LLM**
- ğŸ§¾ Context-grounded answers (no hallucination)
- ğŸ–¥ï¸ Clean Streamlit UI
- ğŸ”— REST API powered by FastAPI

---

## ğŸ› ï¸ Tech Stack

**Frontend**
- Streamlit

**Backend**
- FastAPI
- Uvicorn

**RAG Pipeline**
- LangChain
- ChromaDB (Vector Store)
- Sentence Transformers (Embeddings)

**LLM**
- Groq (ChatGroq)

---

## ğŸ“ Project Structure

multi-doc-rag-chatbot/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ api.py # FastAPI endpoints
â”‚ â”œâ”€â”€ rag.py # RAG logic (ingestion + retrieval)
â”‚ â”œâ”€â”€ uploads/ # Uploaded PDFs
â”‚ â””â”€â”€ vectorstore/ # ChromaDB data
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py # Streamlit UI
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md


---

## ğŸ” Environment Setup

Create a `.env` file in the project root:

GROQ_API_KEY=your_groq_api_key_here

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Create and activate virtual environment
```bash
python -m venv env
source env/bin/activate     # Linux / Mac
env\Scripts\activate        # Windows
```
### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### â–¶ï¸ Running the Application
Start Backend (FastAPI)
```bash
cd backend
uvicorn api:app --host 0.0.0.0 --port 8000
```

Start Frontend (Streamlit)
```bash
cd frontend
streamlit run app.py
```
Open: http://localhost:8501

## ğŸ’¬ How It Works
Upload one or more PDF files
Documents are split into chunks and embedded
Embeddings are stored in ChromaDB
User selects a document and asks a question
Relevant chunks are retrieved
LLM generates an answer using retrieved context and chat history



